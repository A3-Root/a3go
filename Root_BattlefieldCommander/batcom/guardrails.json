{
  "llm_providers": [
    {
      "name": "openai_gpt5_mini",
      "priority": 1,
      "enabled": true,
      "provider": "openai",
      "model": "gpt-5-mini",
      "endpoint": "",
      "api_key": "",
      "timeout": 30,
      "min_interval": 30.0,
      "rate_limit": 5,
      "max_input_tokens": 128000,
      "max_output_tokens": 128000,
      "use_responses_api": true,
      "_comment_use_responses_api": "Use OpenAI Responses API with native prompt caching (recommended for gpt-5/o1 models). Set to false to use Chat Completions API."
    },
    {
      "name": "openai_gpt5_nano",
      "priority": 2,
      "enabled": true,
      "provider": "openai",
      "model": "gpt-5-nano",
      "endpoint": "",
      "api_key": "",
      "timeout": 30,
      "min_interval": 30.0,
      "rate_limit": 5,
      "max_input_tokens": 128000,
      "max_output_tokens": 128000,
      "use_responses_api": true
    },
    {
      "name": "gemini_25_flash_lite",
      "priority": 3,
      "enabled": true,
      "provider": "gemini",
      "model": "gemini-2.5-flash-lite",
      "endpoint": "",
      "_comment_endpoint": "Leave empty for default Google endpoint. Only set if using a custom proxy/gateway.",
      "api_key": "",
      "timeout": 30,
      "min_interval": 30.0,
      "rate_limit": 2,
      "max_input_tokens": 128000,
      "max_output_tokens": 65536,
      "_comment_max_output_tokens": "Maximum tokens for LLM response. Gemini 2.5 Flash supports up to 65536 output tokens.",
      "thinking_enabled": true,
      "_comment_thinking_enabled": "Enable Gemini thinking/reasoning. Set to true to use model's internal reasoning process.",
      "thinking_mode": "openai_compat",
      "_comment_thinking_mode": "Thinking API mode: 'native_sdk' (use Google GenAI SDK) or 'openai_compat' (use OpenAI compatibility endpoint)",
      "thinking_budget": -1,
      "_comment_thinking_budget": "Gemini 2.5 thinking budget in tokens: -1=dynamic (recommended), 0=disabled, 512-24576=explicit token count",
      "thinking_level": "high",
      "_comment_thinking_level": "Gemini 3 thinking level: 'low' or 'high'. Only used with Gemini 3 models.",
      "reasoning_effort": "medium",
      "_comment_reasoning_effort": "OpenAI compatibility mode reasoning effort: 'minimal', 'low', 'medium', 'high', or 'none'",
      "include_thoughts": true,
      "_comment_include_thoughts": "Include thought summaries in response. Set to false to only get final answers without reasoning traces.",
      "log_thoughts_to_file": true,
      "_comment_log_thoughts_to_file": "Log thought summaries to per-AO log files. Set to false to only log to console."
    },
    {
      "name": "anthropic_claude_sonnet",
      "priority": 4,
      "enabled": false,
      "provider": "claude",
      "model": "claude-3-5-sonnet-20240620",
      "endpoint": "",
      "api_key": "YOUR_CLAUDE_API_KEY",
      "timeout": 30,
      "min_interval": 30.0,
      "rate_limit": 2,
      "max_input_tokens": 32000,
      "max_output_tokens": 128000
    },
    {
      "name": "deepseek_chat",
      "priority": 5,
      "enabled": false,
      "provider": "deepseek",
      "model": "deepseek-chat",
      "endpoint": "https://api.deepseek.com",
      "api_key": "YOUR_DEEPSEEK_API_KEY",
      "timeout": 30,
      "min_interval": 30.0,
      "rate_limit": 2,
      "max_input_tokens": 32000,
      "max_output_tokens": 128000
    }
  ],
  "_comment_priority": "Lower priority number = tried first. Fallback goes to next priority if current fails.",
  "_comment_enabled": "Set enabled=true to include in fallback chain. Disabled providers are skipped."
}
